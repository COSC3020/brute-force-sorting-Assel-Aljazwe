[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/7eEMzrNd)
# Brute-Force Sorting

We talked about the complexity of the sorting problem, and used an argument over
all permutations of a list to be sorted to determine its complexity. Implement
a function to sort a list by systematically trying all permutations of the input
list, using the template in `code.js`. Test your new function; I've provided
some basic testing code that uses [jsverify](https://jsverify.github.io/) in
`code.test.js`.

The return value should be the number of permutations that were tried until the
sorted list was "discovered". The unsorted list passed as an argument should be
sorted, i.e. do not copy the list and sort the copy.

## Runtime Analysis

What is the runtime complexity of the algorithm that you implemented? What does
a best case input for your implementation look like, what does a worst case
input look like? How would this complexity change if you generated permutations
randomly without memory instead of systematically trying them?

Describe your reasoning and the conclusion you've come to. Your reasoning is the
most important part. Add your answer to this markdown file.

## Analysis Answer:
The brute-force sorting algorithm, known as permutation sort, sorts an array by generating all possible permutations of the array and checking each to see if it is sorted. This approach seems conceptually simple, but it is very complex in terms of complexity.

### Complexity Analysis:
- **Generating Permutations**: The algorithm uses Heap's algorithm for generating permutations, which generates each permutation in $O(1)$ time after the first permutation. However, the total number of permutations for an array of length $n$ is $n!$ (factorial), leading to a total runtime complexity for generating and checking all permutations as $O(n \cdot n!)$, considering that checking if an array is sorted is $O(n)$ for each permutation.

- **Checking if Sorted**: Each permutation must be checked to see if it is sorted, which is done in linear time, $O(n)$. Since this check is done for each of the $n!$ permutations, the checking contributes significantly to the overall runtime of the algorithm.

### Best and Worst Case Inputs:
- **Best Case**: The best case scenario for this algorithm is when the input array is already sorted. In this case, the algorithm will generate the first permutation (which is the original, already-sorted array), check it, find it sorted and terminate. This results in a best-case runtime complexity of $O(n)$, as it only needs to perform a single sorted check.

- **Worst Case**: The worst case occurs when the sorted permutation is the very last permutation to be generated by the algorithm. Even though the sequence of these checks isn't straightforward (it's not just starting from the reverse order for instance), we still must examine every permutation. This approach means that in the worst-case, sorting takes significantly longer with the complexity being $O(n \cdot n!)$ because all permutations must be checked.

### Impact of Random Permutation Generation: 
If permutations were generated randomly without memory (not checking for duplicates or ensuring all permutations are covered), the algorithm's runtime complexity would become unpredictable. In theory, a sorted permutation could be found quickly, leading to a seemingly efficient runtime. However, without a systematic approach to ensure all permutations are generated, the following occurs:
- **Random Generation Efficiency**: The algorithm might never generate the sorted permutation, especially as $n$ increases, making the probability of randomly hitting the sorted permutation extremely low.
- **Duplication and Coverage**: Generating permutations randomly without memory increases the likelihood of generating the same permutation multiple times while missing others, potentially never generating the sorted permutation at all.

### Conclusion:
The brute-force sorting algorithm implemented has a factorial runtime complexity in the average and worst-case scenarios, making it highly inefficient for sorting arrays of any significant size. The best-case scenario offers linear complexity, but this is highly situational and not practical for general sorting purposes.

Generating permutations randomly without ensuring coverage of all permutations would not practically improve the algorithm's efficiency due to the increased risk of duplication and the possibility of never generating the sorted permutation. The systematic approach, while still factorial in complexity, at least guarantees that the sorted array will be found eventually.
